{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment pyspark scaffold\n",
    "\n",
    "Our engine **must** be coded in this notebook. The platform provides some cool automations to ensure the correct engine life cycle; this is why we are asking you to please use this notebook. Donâ€™t panic! You can find some useful guidelines in the sections below. If you want to use this example you must **copy** it to the **nootebooks folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exampleenginepythonqiyhbwvw.business.business_logic import BusinessLogic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENT_VARS_PM\n",
    "INPUTCLIENTFILE = \"/data/sandboxes/test/data/clients.csv\"\n",
    "INPUTCONTRACTFILE = \"/data/sandboxes/test/data/contracts.csv\"\n",
    "INPUTPRODUCTFILE = \"/data/sandboxes/test/data/products.csv\"\n",
    "OUTPUTFILE = \"/data/sandboxes/test/data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data #\n",
    "Reading input and output paths and create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_df = spark.read.csv(INPUTCLIENTFILE, header=True)\n",
    "contracts_df = spark.read.csv(INPUTCONTRACTFILE, header=True)\n",
    "products_df = spark.read.csv(INPUTPRODUCTFILE, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bussines Logic #\n",
    "Filtering Clients Dataframe depending on age and if Client is or not VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_logic = BusinessLogic()\n",
    "clients_df = business_logic.filter_example(clients_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Join between already filtered Clients and Contracts Dataframes by \"cod_client\"/\"cod_titular\". \n",
    "Spark Broadcast Join between previous join Dataframe and products by \"cod_product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_contracts_df = business_logic.join_example(clients_df, contracts_df, products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Query for filtering Clients with more than 3 contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_contracts_df.createOrReplaceTempView(\"clients_contracts_df\")\n",
    "clients_target_df = business_logic.filter_sql_example(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new hash column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_target_df = business_logic.add_hash(clients_target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Dataframe for future operations with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_target_df_cache = clients_target_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Data #\n",
    "Writing final Dataframe in parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUTFILE != \"\":\n",
    "    clients_target_df_cache.write.parquet(OUTPUTFILE, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_logic.send_notification(clients_target_df_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enma convert #\n",
    "The enma-sdk library provides the convert functionality. In order to make use of it you need to have the library installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install enma31==2.0.1 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed you can make use of it from a notebook. For example in our notebook it includes at the end the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way the enma.convert function will generate an `experiment.py` file with the content of the notebook passed as first parameter, in our case `engine.ipynb` in that same path, if you want it to generate it somewhere else you can specify the path in the second parameter of the function. By default it generates it in the same path as the notebook file passed as the first parameter.\n",
    "When executing the enma.convert cell, a button panel with two buttons will appear:\n",
    "* **Validate**: which will pass a pep8 rules autoformatter, automatically resolving all the errors it can and showing the ones that need manual correction by the user. It will also show certain warnings about other validations such as avoid prints, .show()...\n",
    "* **Export**: Once the code has been left error free (with or without warnings) it will generate two files:\n",
    "  * `application.conf`: from the variables defined in the cell with `#ENVIRONMENT_VARS_PM`.\n",
    "  * `experiment.py`: script with the contents of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enma.convert(\"engine.ipynb\",\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the notebook is validated we must copy the file experiment.py to the repository module `exampleenginepythonqiyhbwvw/` folder and the `application.conf` file to the `resources/`folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![[ `basename $PWD` == \"notebooks\" ]] && mv experiment.py ../exampleenginepythonqiyhbwvw && mv application.conf ../resources/ && echo \"Files moved succesfully\" || echo \"Failed. Please move the files manually.\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02c6acaa5d07b09dc53727053eb93a775de9815ef9faf936dabaf25417ab83fd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
